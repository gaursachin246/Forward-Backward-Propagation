{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2462fdfd-c594-4d2b-9400-85fb44a63f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Purpose of Forward Propagation\n",
    "\n",
    "#Forward propagation is the process of passing input data through a neural network to obtain output predictions. Its purpose is to:\n",
    "\n",
    "\n",
    "# Calculate the output of each layer\n",
    "#2. Compute the loss between predictions and actual labels\n",
    "#3. Evaluate the network's performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67fbcc36-c097-4a55-a60e-58e83c501c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Mathematical Implementation in Single-Layer Feedforward Neural Network\n",
    "\n",
    "#Forward propagation in a single-layer feedforward neural network is implemented mathematically as:\n",
    "\n",
    "\n",
    "#1. Input Layer: x (input vector)\n",
    "#2. Weighted Sum: z = w^T x + b (dot product of weights and inputs plus bias)\n",
    "#3. Activation Function: a = σ(z) (activation function applied to weighted sum)\n",
    "#4. Output Layer: y = a (predicted output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3341993d-87bb-4090-b5e6-e050f478cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Role of Activation Functions\n",
    "\n",
    "#Activation functions introduce non-linearity to the network, allowing it to learn complex relationships. Common activation functions include:\n",
    "\n",
    "\n",
    "#1. Sigmoid (σ(x) = 1 / (1 + exp(-x)))\n",
    "#2. ReLU (Rectified Linear Unit)\n",
    "#3. Tanh (Hyperbolic Tangent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66f1913a-79de-48ff-acbe-8d1fbdc2402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Weights and Biases in Forward Propagation\n",
    "\n",
    "#Weights and biases are learnable parameters that adjust during training:\n",
    "\n",
    "\n",
    "#1. Weights (w): scale the importance of inputs\n",
    "#2. Biases (b): shift the activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10578d29-836c-41b0-bb82-ba1ceb1e9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5. Softmax Function in Output Layer\n",
    "\n",
    "#The softmax function is applied in the output layer for multi-class classification problems:\n",
    "\n",
    "\n",
    "#1. Normalizes output probabilities to ensure they sum to 1\n",
    "#2. Computes probability distribution over classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aee9b752-6fe9-4745-85de-fff976aa7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Purpose of Backward Propagation\n",
    "\n",
    "#Backward propagation is the process of computing gradients to update weights and biases during training:\n",
    "\n",
    "\n",
    "#1. Computes error gradients between predictions and actual labels\n",
    "#2. Propagates gradients backward through the network\n",
    "#3. Updates weights and biases using optimization algorithms\n",
    "\n",
    "\n",
    "#By combining forward and backward propagation, neural networks can learn from data and improve their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d06287-8fb8-4ce0-9bfd-ef74fb8fb72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
